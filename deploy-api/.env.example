# Copy to .env and fill in your values.
# Never commit .env to version control.

# ── Required ─────────────────────────────────────────────────
GCP_PROJECT_ID=your-gcp-project-id

# RunPod Serverless Template ID (create at runpod.io → Serverless → Templates)
RUNPOD_TEMPLATE_ID=your-template-id

# Docker image for the inference worker
DOCKER_IMAGE=your-org/inference:latest

# ── Internal webhook callback ────────────────────────────────
# Set to your Cloud Run service URL after first deploy
INTERNAL_WEBHOOK_BASE_URL=https://YOUR_SERVICE-hash-uc.a.run.app
INTERNAL_WEBHOOK_SECRET=change-me-strong-random-secret

# ── Firestore collection names ───────────────────────────────
FIRESTORE_COLLECTION_DEPLOYMENTS=visgate_deploy_api_deployments
FIRESTORE_COLLECTION_LOGS=visgate_deploy_api_logs
FIRESTORE_COLLECTION_API_KEYS=visgate_deploy_api_api_keys

# ── Optional: S3/R2/MinIO model cache ────────────────────────
# Leave empty to disable (models downloaded from HF on every cold start)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_ENDPOINT_URL=              # for R2: https://ACCOUNT.r2.cloudflarestorage.com
S3_MODEL_URL=                  # e.g. s3://my-bucket/models/sd-turbo

SHARED_CACHE_ENABLED=true
SHARED_CACHE_ALLOWED_MODELS=stabilityai/sd-turbo,black-forest-labs/FLUX.1-schnell,stabilityai/stable-diffusion-xl-base-1.0,stabilityai/sdxl-turbo,stabilityai/stable-diffusion-3.5-large
SHARED_CACHE_REJECT_UNLISTED=true

# ── RunPod cost tuning ───────────────────────────────────────
# 0 = no idle GPU cost (cold start ~40-80s)
# 1 = one always-warm worker (~$0.35-0.80/hr idle per endpoint)
RUNPOD_WORKERS_MIN=0
RUNPOD_WORKERS_MAX=3
RUNPOD_IDLE_TIMEOUT_SECONDS=120
RUNPOD_DEFAULT_LOCATIONS=US

# ── Logging ──────────────────────────────────────────────────
LOG_LEVEL=INFO
